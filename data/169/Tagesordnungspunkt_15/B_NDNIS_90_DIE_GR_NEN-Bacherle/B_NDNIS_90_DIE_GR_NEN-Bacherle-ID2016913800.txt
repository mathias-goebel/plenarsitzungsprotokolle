Sehr geehrter Herr Präsident! Liebe Kolleginnen und Kollegen! Zu der Frage eines Drohnenflottenaufbaus haben die Kollegen Faber, Krämer und Arlt sehr vieles gesagt. Ich möchte auf einen Aspekt eingehen, den Sie in Ihrem Antrag auch ansprechen, nämlich den Einsatz von KI-Systemen, wo Sie auch von vollautonomen Waffensystemen sprechen, weil ich ein kleines bisschen irritiert bin, wie nonchalant Sie das fordern, insbesondere vor dem Hintergrund, dass Ihre Parteikollegin Ursula von der Leyen gerade den AI Act zum Abschluss gebracht hat – eine klare Regulierung für KI-Systeme im zivilen Bereich –, in dem es ganz klare risikobasierte Einstufungen von KI-basierter Technologie gibt.
Jetzt führen wir hier die Debatte über Drohnen. Es ist vollkommen richtig, dass diese im Krieg der Russen gegen die Ukraine eine neue Relevanz gewonnen haben und dass man das beobachten muss. Der Kollege Arlt hat auch schon angesprochen, wie wichtig KI bei der Datenverarbeitung, bei der Unterstützung, bei der Erzeugung von Lagebildern sein kann. Aber je mehr wir uns dann dem Einsatz von Waffensystemen, insbesondere von letalen Waffensystemen, annähern, desto schwieriger wird natürlich auch – ich sage das ganz ehrlich – die moralische Dimension und die Frage der Verantwortung.
Und die klammern Sie im Prinzip in Ihrem Antrag leider fast vollkommen aus bzw. sagen an einer Stelle: Da soll man sich ein bisschen drum kümmern. – Das finde ich aber angesichts der Möglichkeiten solcher Systeme absolut nicht angemessen.
Ich möchte ein sehr simplifiziertes Beispiel machen: Wir haben eine Soldatin, die an einem Posten sitzt und ein Lagebild bekommt, das ihr anzeigt, dass sich zwei Fahrzeuge nähern. Die Einschätzung des mit künstlicher Intelligenz erzeugten Lagebilds ist: Das sind keine Krankenwägen, das sind voraussichtlich feindliche Einsatzfahrzeuge. Jetzt kann man mit dieser Information weiterarbeiten. Wenn wir jetzt aber warnen wollen oder die Frage klären wollen: „Wie gehen wir damit um?“, ist es unglaublich wichtig, dass wir wissen: Wie ist eigentlich diese KI trainiert worden? Mit welchem Datensatz? Wenn wir uns zum Beispiel in einem arabischsprachigen Land befinden, unsere KI aber nur mit deutschen Krankenwägen trainiert haben, also nur – ich mache es jetzt mal ganz einfach – mit solchen, die mit dem Roten Kreuz gekennzeichnet sind und nicht mit dem Roten Halbmond, dann kommt eine Verzerrung, eine falsche Aussage zustande.
Das ist in vielerlei Hinsicht vielleicht gar nicht so relevant. In dem Moment, wo es aber um Leben und Tod geht, ist es hochrelevant.
Auf diese Frage müssten Sie doch, wenn Sie sagen, Sie wollten vollautonome Waffensysteme, viel deutlicher und elaborierter eingehen. Denn auch die Frage von Human-in-the-Loop muss mehr sein als nur: Es gibt jemanden, der auf einen Knopf drückt. Es muss jemand sein, der sich weitere Parameter und weitere Entscheidungsfindungsdimensionen anzeigen lassen kann, weil am Ende diese Person einen großen Teil der Verantwortung trägt bzw. am Ende die Bundeswehr diese Verantwortung trägt.
Das klammern Sie leider aus.