Herr Präsident! Meine Damen und Herren! Der Einsatz von algorithmischen Systemen birgt viele Chancen und Potenziale – ohne Frage. Aber er bringt auch gewisse Schwierigkeiten mit sich, zum Beispiel Probleme und Gefahren zu erkennen, aufzudecken oder gar nachzuweisen. Nehmen wir das Beispiel, dass die KI ein MRT-Bild analysiert. Im Zweifel können wir nicht nachverfolgen, warum das Ergebnis so ausfällt und nicht anders. Hinzu kommt, dass uns die Geschwindigkeit und Dynamik in der Entwicklung neuer Anwendungsfälle für KI vor Herausforderungen stellt. Das ist im digitalen Zeitalter alles nicht neu, aber bisher haben wir noch keine gute Antwort darauf gefunden.
Insofern wundert es mich nicht, dass wir heute, nach zwei Jahren Arbeit, zwar einen Konsens vorliegen haben, aber nach wie vor unterschiedliche Haltungen ausgeprägt sind. Deshalb haben wir für uns ein Sondervotum eingebracht – dafür einen großen Dank an Jan Kuhlen und den wundervollen Verein D64 –, in dem wir festgehalten haben, dass die Regulierung zuerst das Risiko ihrer Anwendung für Gesellschaft und Individuen berücksichtigen muss und danach erst die Anforderungen zu definieren sind.
Das heißt: Sobald KI im autonomen Fahrzeug steckt, hat das Konsequenzen für ziemlich viele Verkehrsteilnehmer, und wenn die Auswahl von Nachrichten durch KI beeinflusst wird, dann hat das sogar Konsequenzen für unsere politische Ordnung. In diesen Fällen müssen wir dafür sorgen, dass die Grundsätze unseres Miteinanders nicht ausgehebelt werden.
Das bedeutet konkret: Der Gesetzgeber macht die Vorgaben, nicht die Technik.
Wir halten daher eine gestufte Regulierung, die je nach konkretem Risiko mehrere Abstufungen vorsieht, für den richtigen Ansatz; denn nur so können wir Risiken spezifisch regulieren. Schließlich macht es einen kleinen Unterschied, ob wir über Instagram-Filter oder über autonome Waffen sprechen; das sind unterschiedliche Risikointensitäten, und das gebietet eine unterschiedliche Einstufung und eine unterschiedliche Behandlung.
Gleichzeitig darf es natürlich nicht zu regulatorischen Schnellschüssen kommen. Daher schlagen wir vor, das Risiko tastend zu ermitteln. Voraussetzung dafür ist, dass derjenige, der algorithmische Systeme einsetzt, dies transparent macht. Ziel ist ein Prozess, der eine fundierte, evidenzbasierte Grundlage für die herantastende Ausgestaltung von Anforderungen – zum Beispiel im Hinblick auf die menschliche Arbeit, Robustheit, Genauigkeit – ermöglicht. Dabei sind durch Normen und Standards grundrechtlich geschützte Rechtsgüter zu wahren. Schließlich sind Selbstbestimmung, Handlungsfreiheit, Meinungs- und Informationsfreiheit sowie Versammlungsfreiheit für uns oberste Maxime.
Diese Bedingungen, unter denen wir den Einsatz algorithmischer Systeme für Menschen rechtfertigen können, finden sich übrigens auch in der DSGVO. Ich möchte einmal betonen: Die DSGVO schützt nicht Daten, sondern sie schützt Menschen. Insofern habe ich einige Sondervoten mit ziemlicher Verwunderung gelesen, die mit der Zweckbindung oder der Datenminimierung zwei Grundsätze der DSGVO streichen wollen. Die DSGVO ist mittlerweile ein Werk, das globale Anerkennung erfährt, und deren Standards als europäische Errungenschaft gelten.
Europäische KI kann sicher viel – aber am Datenschutz sparen, das kann sie mit Sicherheit nicht.
Heute haben wir schon den einen oder anderen Dank gehört. Ein Dank ist bislang jedoch ausgeblieben, und deshalb möchte ich mich einreihen und einen Dank an die Vorsitzende der Enquete-Kommission aussprechen, die uns mit ganz viel Engagement durch diese zwei Jahre geführt hat: Vielen Dank, Daniela Kolbe!