Frau Präsidentin! Meine Damen und Herren! Ja, in der Tat: Der vorliegende Bericht des Büros für Technikfolgenabschätzung zu Algorithmen in digitalen
                    Medien greift ein wichtiges Thema auf; war ja auch ein Auftrag aus dem Bundestag. Er vertieft und bestätigt zugleich vieles, womit wir uns in der Projektgruppe
                    „KI und Medien“ der Enquete-Kommission „Künstliche Intelligenz“ in der letzten Wahlperiode schon sehr aufwendig und umfänglich beschäftigt haben. Wir haben ja
                    damals genau diese Fragen über verschiedene Projektgruppen hinweg untersucht.
Lassen Sie mich einige aus meiner Sicht zentrale Schlüsse ziehen; da kann ich auch an den ein oder anderen Redner hier sehr gut anschließen.
Erstens. Es gibt – dies stellt der Bericht ausdrücklich fest – weiteren Forschungsbedarf über die Wirkung von Algorithmen auf die Meinungsbildung.
                    Aber hier brauchen wir vor allem besseren Zugang für die Wissenschaft zu Daten der Plattformen. Dazu gibt es im Digital Services Act, der EU-Verordnung über die
                    Pflichten digitaler Dienste, die ja gerade in Umsetzung und Diskussion ist, erste, aber eben nicht hinreichende Ansätze.
Zweitens. Soviel man hier über die Wirkung von Algorithmen diskutieren kann: Die Hauptursache unserer Probleme ist kein Rätsel. Wir haben zentrale
                    Rollen im Medienwandel profitorientierten Monopolisten überlassen, und dementsprechend sieht auch die Medienlandschaft aus. Hier müssen wir gegensteuern.
Das bedeutet drittens – Herr Grundl hat es vorhin auch schon ausgeführt – Regulierung, und auch hier sind Anfänge gemacht. Wie das in der Praxis
                    wirkt, das muss sich zeigen, das muss aber eben auch weiter untersucht werden. Klar ist: Die großen Plattformmonopole müssen in ihre Schranken verwiesen
                    werden.
Objektive und wahrheitsgemäße Berichterstattung sollen Meinungsbildung grundieren. Klar ist ebenso: Regulierung darf nun wiederum nicht dazu führen,
                    dass sich der Staat zum obersten Medienkontrolleur selbst ernennt.
Nicht zuletzt deshalb sollten wir, statt nur Bestehendes zu regulieren, viertens auch neue demokratische Ansätze und Alternativen stärken.
So ist zum Beispiel – da gebe ich der Kollegin vollkommen recht; ich weiß jetzt gar nicht genau, wer es gesagt hat – die Definition des digitalen
                    Auftrags des öffentlich-rechtlichen Rundfunks überfällig. Damit entstünden gerade in Zeiten der Vertrauenskrisen neue Möglichkeiten der Mitsprache, der
                    Mitbestimmung, also neue Chancen. Und dezentrale nichtkommerzielle Modelle wie Mastodon
bieten attraktive Zukunftsvisionen als Kommunikationsplattformen, die eben nicht den durchgeknallten Launen von irgendwelchen Milliardären
                    ausgeliefert sind.
Schließlich: Wir müssen auch die weitere technische Entwicklung kontinuierlich beobachten und darauf reagieren.
– Ja, das Kompetenzzentrum spricht, ich weiß.
Derzeit sind Sprachmodelle wie ChatGPT in aller Munde. Bereits die KI-Enquete-Kommission hat klar gesagt: Hier braucht es eine Kennzeichnungspflicht
                    bei automatisiert erstellten Texten. Gut so, dass das Büro für Technikfolgenabschätzung in diesen Tagen auch ein Konzept vorgelegt hat, wie wir hier in unserer
                    Diskussion mit ChatGPT weiter umgehen können.
Jawoll. – Hier wie anderswo gilt für Medienpolitik: Wenn es technologische Entwicklungen gibt, dann muss sich der Bundestag auch –
– in technologischen Fragen selbst qualifizieren. Das gilt auch für Herrn Renner.Frau Präsidentin! Meine Damen und Herren! Ja, in der Tat: Der vorliegende Bericht des Büros für Technikfolgenabschätzung zu Algorithmen in digitalen
                    Medien greift ein wichtiges Thema auf; war ja auch ein Auftrag aus dem Bundestag. Er vertieft und bestätigt zugleich vieles, womit wir uns in der Projektgruppe
                    „KI und Medien“ der Enquete-Kommission „Künstliche Intelligenz“ in der letzten Wahlperiode schon sehr aufwendig und umfänglich beschäftigt haben. Wir haben ja
                    damals genau diese Fragen über verschiedene Projektgruppen hinweg untersucht.
Lassen Sie mich einige aus meiner Sicht zentrale Schlüsse ziehen; da kann ich auch an den ein oder anderen Redner hier sehr gut anschließen.
Erstens. Es gibt – dies stellt der Bericht ausdrücklich fest – weiteren Forschungsbedarf über die Wirkung von Algorithmen auf die Meinungsbildung.
                    Aber hier brauchen wir vor allem besseren Zugang für die Wissenschaft zu Daten der Plattformen. Dazu gibt es im Digital Services Act, der EU-Verordnung über die
                    Pflichten digitaler Dienste, die ja gerade in Umsetzung und Diskussion ist, erste, aber eben nicht hinreichende Ansätze.
Zweitens. Soviel man hier über die Wirkung von Algorithmen diskutieren kann: Die Hauptursache unserer Probleme ist kein Rätsel. Wir haben zentrale
                    Rollen im Medienwandel profitorientierten Monopolisten überlassen, und dementsprechend sieht auch die Medienlandschaft aus. Hier müssen wir gegensteuern.
Das bedeutet drittens – Herr Grundl hat es vorhin auch schon ausgeführt – Regulierung, und auch hier sind Anfänge gemacht. Wie das in der Praxis
                    wirkt, das muss sich zeigen, das muss aber eben auch weiter untersucht werden. Klar ist: Die großen Plattformmonopole müssen in ihre Schranken verwiesen
                    werden.
Objektive und wahrheitsgemäße Berichterstattung sollen Meinungsbildung grundieren. Klar ist ebenso: Regulierung darf nun wiederum nicht dazu führen,
                    dass sich der Staat zum obersten Medienkontrolleur selbst ernennt.
Nicht zuletzt deshalb sollten wir, statt nur Bestehendes zu regulieren, viertens auch neue demokratische Ansätze und Alternativen stärken.
So ist zum Beispiel – da gebe ich der Kollegin vollkommen recht; ich weiß jetzt gar nicht genau, wer es gesagt hat – die Definition des digitalen
                    Auftrags des öffentlich-rechtlichen Rundfunks überfällig. Damit entstünden gerade in Zeiten der Vertrauenskrisen neue Möglichkeiten der Mitsprache, der
                    Mitbestimmung, also neue Chancen. Und dezentrale nichtkommerzielle Modelle wie Mastodon
bieten attraktive Zukunftsvisionen als Kommunikationsplattformen, die eben nicht den durchgeknallten Launen von irgendwelchen Milliardären
                    ausgeliefert sind.
Schließlich: Wir müssen auch die weitere technische Entwicklung kontinuierlich beobachten und darauf reagieren.
– Ja, das Kompetenzzentrum spricht, ich weiß.
Derzeit sind Sprachmodelle wie ChatGPT in aller Munde. Bereits die KI-Enquete-Kommission hat klar gesagt: Hier braucht es eine Kennzeichnungspflicht
                    bei automatisiert erstellten Texten. Gut so, dass das Büro für Technikfolgenabschätzung in diesen Tagen auch ein Konzept vorgelegt hat, wie wir hier in unserer
                    Diskussion mit ChatGPT weiter umgehen können.
Jawoll. – Hier wie anderswo gilt für Medienpolitik: Wenn es technologische Entwicklungen gibt, dann muss sich der Bundestag auch –
– in technologischen Fragen selbst qualifizieren. Das gilt auch für Herrn Renner.