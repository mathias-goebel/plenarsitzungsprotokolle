Sehr geehrter Herr Präsident! Liebe Kolleginnen und Kollegen! Ich habe mich mit wirklich großem Enthusiasmus in die Arbeit bei der Enquete-Kommission „Künstliche Intelligenz“ gestürzt. Ich habe Transparenz und Bürgerbeteiligung erwartet; denn KI-Computerprogramme, die selbst lernen und Entscheidungen treffen können, werden unsere Gesellschaft sehr verändern. Deshalb sollte die Gesellschaft auch daran beteiligt sein, wenn diese Kommission Handlungsempfehlungen für die Politik erarbeitet, die sich ja auf uns alle auswirken werden. Stattdessen fand Transparenz nur sehr wenig statt, und Bürgerbeteiligung fand bisher überhaupt nicht statt.
Die drei Projektgruppen – Wirtschaft, Gesundheit und Staat – tagten ausschließlich hinter geschlossenen Türen. Sitzungen der Gesamtkommission waren nur während der Kurzvorträge von Sachverständigen öffentlich. Für ihre anschließende Befragung wurde der Livestream einfach abgeschaltet, und die Gäste wurden rausgeschickt. Über Stunden hieß es: Gäste rein und raus, rein und raus. – Ein wirklich absurder Vorgang!
Unsere Anträge, die drei abgeschlossenen Teilberichte zu veröffentlichen, wurden abgelehnt.
Nur das jeweils erste Kapitel wurde gestern öffentlich.
Der Rest wird nun ein Jahr lang in der Schublade schmoren und dabei ganz bestimmt nicht an Aktualität gewinnen.
Die Planung für die Bürgerbeteiligung wiederum zog sich so lange hin, dass es nur noch eine Feigenblattbeteiligung im Frühjahr 2020 geben wird, bei der die Zivilgesellschaft ausgewählte Ergebnisse der Enquete noch bewerten kann. Ihr Input kann zu diesem Zeitpunkt aber kaum noch einfließen.
So baut man kein Vertrauen auf, liebe GroKo, und ernstgemeinte Bürgerbeteiligung geht anders.
Eine der drei Arbeitsgruppen – Künstliche Intelligenz und Staat – habe ich geleitet. Da die Vorstellungen der Linksfraktion im Bericht der AG nicht ausreichend enthalten sind, werden wir für die Themenbereiche „Innere Sicherheit“ und „Militär“ ein Sondervotum veröffentlichen.
Nach unserer Überzeugung muss es rote Linien für den Einsatz von KI geben, nämlich immer dann, wenn Fehlentscheidungen der selbstlernenden Systeme einen unvertretbaren Schaden für Menschen oder die Gesellschaft als Ganzes nach sich ziehen. Diese roten Linien müssen mit einem Risikoklassenmodell gezogen werden und sind in jedem Fall zu respektieren, auch dann, wenn die Klassifizierung ein Einsatzverbot bedeutet. Bei Grundrechtseingriffen wie beim Einsatz biometrischer Gesichtserkennung im öffentlichen Raum oder beim Einsatz autonomer Waffen sehe ich diese roten Linien überschritten.
Der Bericht zum Thema „Künstliche Intelligenz und Staat“ ist so leider ein GroKo-Positionspapier geworden, das gerade bei sensiblen Fragen eine ausreichend klare Haltung vermissen lässt. Unser Anspruch als Linksfraktion ist ein anderer.
Im Übrigen bin ich der Meinung, dass Informationen zu Schwangerschaftsabbrüchen nichts im Strafgesetzbuch verloren haben. § 219a gehört abgeschafft.
Vielen Dank.